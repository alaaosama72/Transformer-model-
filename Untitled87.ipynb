{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9GTryujsRsV06mkFrBgHH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alaaosama72/Transformer-model-/blob/main/Untitled87.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJne7NWtJcMO",
        "outputId": "77600ce0-a67b-43c7-84cd-c0930ff24fa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Scores: tensor([[[[0.0959, 0.0968, 0.0929, 0.1039, 0.1030, 0.1051, 0.0976, 0.0962,\n",
            "           0.0996, 0.1089],\n",
            "          [0.0948, 0.0953, 0.0955, 0.1038, 0.0980, 0.1042, 0.0953, 0.0968,\n",
            "           0.1062, 0.1102],\n",
            "          [0.0943, 0.0975, 0.0977, 0.1016, 0.1024, 0.1025, 0.0984, 0.0987,\n",
            "           0.0984, 0.1086],\n",
            "          [0.1018, 0.0930, 0.0993, 0.1042, 0.1021, 0.1033, 0.0974, 0.0973,\n",
            "           0.0968, 0.1047],\n",
            "          [0.0989, 0.0939, 0.0952, 0.1061, 0.1035, 0.1041, 0.0940, 0.0964,\n",
            "           0.0951, 0.1129],\n",
            "          [0.0996, 0.0962, 0.0954, 0.1051, 0.1030, 0.1049, 0.0986, 0.0984,\n",
            "           0.0924, 0.1065],\n",
            "          [0.0937, 0.1003, 0.0998, 0.1005, 0.0996, 0.1011, 0.0976, 0.0957,\n",
            "           0.1018, 0.1099],\n",
            "          [0.0978, 0.1001, 0.0908, 0.1010, 0.0994, 0.1055, 0.1013, 0.0981,\n",
            "           0.1028, 0.1032],\n",
            "          [0.0932, 0.0955, 0.0970, 0.1044, 0.1002, 0.1024, 0.0943, 0.1014,\n",
            "           0.0999, 0.1118],\n",
            "          [0.0979, 0.0977, 0.0913, 0.1053, 0.0984, 0.1062, 0.0937, 0.1016,\n",
            "           0.0987, 0.1091]],\n",
            "\n",
            "         [[0.1113, 0.0998, 0.1013, 0.0938, 0.1009, 0.0992, 0.0973, 0.0970,\n",
            "           0.0897, 0.1096],\n",
            "          [0.1111, 0.0992, 0.1055, 0.0938, 0.0980, 0.1042, 0.1005, 0.0970,\n",
            "           0.0914, 0.0992],\n",
            "          [0.1053, 0.1037, 0.1100, 0.0927, 0.1020, 0.0992, 0.0977, 0.0943,\n",
            "           0.0931, 0.1020],\n",
            "          [0.1044, 0.1035, 0.1075, 0.0876, 0.1077, 0.0987, 0.0969, 0.0917,\n",
            "           0.0917, 0.1104],\n",
            "          [0.1058, 0.1000, 0.1028, 0.0893, 0.1064, 0.0981, 0.1006, 0.0952,\n",
            "           0.0923, 0.1096],\n",
            "          [0.1090, 0.0931, 0.1022, 0.0908, 0.1079, 0.1008, 0.0972, 0.0921,\n",
            "           0.0965, 0.1105],\n",
            "          [0.1119, 0.1007, 0.1037, 0.0924, 0.1019, 0.0977, 0.0974, 0.0960,\n",
            "           0.0960, 0.1022],\n",
            "          [0.1098, 0.0977, 0.1031, 0.0933, 0.1013, 0.1008, 0.0986, 0.0966,\n",
            "           0.0902, 0.1086],\n",
            "          [0.1113, 0.1033, 0.1017, 0.0890, 0.1061, 0.0995, 0.0980, 0.0933,\n",
            "           0.0917, 0.1061],\n",
            "          [0.1111, 0.0966, 0.1009, 0.0939, 0.0998, 0.0998, 0.0998, 0.1001,\n",
            "           0.0909, 0.1071]],\n",
            "\n",
            "         [[0.1088, 0.0990, 0.0958, 0.1035, 0.0989, 0.0965, 0.0972, 0.1019,\n",
            "           0.0993, 0.0992],\n",
            "          [0.1089, 0.1039, 0.1030, 0.1007, 0.1002, 0.0932, 0.0921, 0.1024,\n",
            "           0.1006, 0.0949],\n",
            "          [0.1153, 0.1010, 0.0954, 0.1056, 0.1029, 0.0929, 0.0929, 0.1007,\n",
            "           0.0979, 0.0954],\n",
            "          [0.1072, 0.1068, 0.1019, 0.1039, 0.0998, 0.0943, 0.0923, 0.0946,\n",
            "           0.1018, 0.0974],\n",
            "          [0.1034, 0.1048, 0.0986, 0.1029, 0.1039, 0.0906, 0.0959, 0.0967,\n",
            "           0.1038, 0.0993],\n",
            "          [0.1090, 0.1036, 0.0947, 0.1038, 0.0976, 0.0940, 0.0966, 0.0989,\n",
            "           0.0998, 0.1020],\n",
            "          [0.1097, 0.1003, 0.0987, 0.1051, 0.0987, 0.0885, 0.0961, 0.0976,\n",
            "           0.1059, 0.0994],\n",
            "          [0.1080, 0.0986, 0.0978, 0.1040, 0.1020, 0.0941, 0.0951, 0.1039,\n",
            "           0.1009, 0.0956],\n",
            "          [0.1097, 0.1066, 0.0961, 0.1054, 0.1008, 0.0901, 0.0942, 0.0999,\n",
            "           0.1018, 0.0954],\n",
            "          [0.1063, 0.1021, 0.0997, 0.1073, 0.0947, 0.0927, 0.0981, 0.0960,\n",
            "           0.1014, 0.1018]],\n",
            "\n",
            "         [[0.0957, 0.0923, 0.1002, 0.1062, 0.1037, 0.0947, 0.1006, 0.0988,\n",
            "           0.1027, 0.1050],\n",
            "          [0.0942, 0.0913, 0.0987, 0.1111, 0.0966, 0.0915, 0.1042, 0.1002,\n",
            "           0.1078, 0.1043],\n",
            "          [0.0961, 0.0963, 0.0998, 0.1016, 0.1013, 0.0954, 0.1010, 0.0950,\n",
            "           0.1046, 0.1088],\n",
            "          [0.0936, 0.0987, 0.1013, 0.1029, 0.1010, 0.0929, 0.0992, 0.0949,\n",
            "           0.1054, 0.1102],\n",
            "          [0.0974, 0.0927, 0.0930, 0.1015, 0.1062, 0.0985, 0.1027, 0.0965,\n",
            "           0.1022, 0.1093],\n",
            "          [0.0929, 0.0919, 0.0960, 0.0987, 0.1064, 0.0932, 0.1032, 0.1066,\n",
            "           0.1075, 0.1035],\n",
            "          [0.0961, 0.0950, 0.0992, 0.1010, 0.1030, 0.0932, 0.0980, 0.0992,\n",
            "           0.1074, 0.1079],\n",
            "          [0.0979, 0.0932, 0.0994, 0.1033, 0.1039, 0.0933, 0.1008, 0.1006,\n",
            "           0.1041, 0.1037],\n",
            "          [0.0956, 0.0925, 0.0996, 0.1027, 0.1077, 0.0939, 0.1012, 0.0944,\n",
            "           0.1014, 0.1109],\n",
            "          [0.0950, 0.0920, 0.0976, 0.1035, 0.1038, 0.0936, 0.1026, 0.1008,\n",
            "           0.1065, 0.1046]],\n",
            "\n",
            "         [[0.1009, 0.0985, 0.1047, 0.1009, 0.0997, 0.0985, 0.0977, 0.0941,\n",
            "           0.1014, 0.1036],\n",
            "          [0.1002, 0.1025, 0.1038, 0.0894, 0.0992, 0.0979, 0.1059, 0.0951,\n",
            "           0.1024, 0.1036],\n",
            "          [0.0943, 0.1046, 0.1022, 0.0996, 0.0981, 0.0982, 0.1012, 0.0954,\n",
            "           0.0993, 0.1072],\n",
            "          [0.1018, 0.0990, 0.1050, 0.0956, 0.0980, 0.0977, 0.1041, 0.0895,\n",
            "           0.1030, 0.1063],\n",
            "          [0.0924, 0.1007, 0.0988, 0.1007, 0.1011, 0.1027, 0.1001, 0.0903,\n",
            "           0.1035, 0.1096],\n",
            "          [0.1032, 0.1023, 0.1014, 0.0980, 0.0987, 0.1014, 0.1002, 0.0919,\n",
            "           0.0994, 0.1037],\n",
            "          [0.0987, 0.0972, 0.1078, 0.0998, 0.0965, 0.1017, 0.1037, 0.0937,\n",
            "           0.1026, 0.0983],\n",
            "          [0.0968, 0.1040, 0.1049, 0.0974, 0.0972, 0.0931, 0.1059, 0.0936,\n",
            "           0.0998, 0.1073],\n",
            "          [0.0951, 0.1040, 0.1080, 0.0994, 0.0971, 0.1010, 0.1020, 0.0870,\n",
            "           0.1044, 0.1020],\n",
            "          [0.1007, 0.0995, 0.1042, 0.0962, 0.0981, 0.0981, 0.1046, 0.0947,\n",
            "           0.0990, 0.1048]],\n",
            "\n",
            "         [[0.0957, 0.0935, 0.0999, 0.0981, 0.1020, 0.1070, 0.1067, 0.0944,\n",
            "           0.1029, 0.0999],\n",
            "          [0.1037, 0.0955, 0.0986, 0.0964, 0.1002, 0.1051, 0.1024, 0.0914,\n",
            "           0.1014, 0.1052],\n",
            "          [0.0954, 0.1029, 0.1012, 0.0922, 0.1003, 0.0986, 0.1121, 0.0942,\n",
            "           0.0992, 0.1038],\n",
            "          [0.0967, 0.0942, 0.0987, 0.0897, 0.1024, 0.1037, 0.1131, 0.0942,\n",
            "           0.1001, 0.1071],\n",
            "          [0.0996, 0.0899, 0.1004, 0.0923, 0.1034, 0.1011, 0.1090, 0.0912,\n",
            "           0.1028, 0.1103],\n",
            "          [0.0941, 0.0989, 0.1027, 0.0897, 0.1045, 0.0958, 0.1134, 0.0923,\n",
            "           0.1002, 0.1084],\n",
            "          [0.0967, 0.0969, 0.1022, 0.0899, 0.0983, 0.0992, 0.1121, 0.0971,\n",
            "           0.1026, 0.1050],\n",
            "          [0.0992, 0.0977, 0.1016, 0.0958, 0.1049, 0.0982, 0.1016, 0.0946,\n",
            "           0.1030, 0.1034],\n",
            "          [0.0976, 0.0937, 0.1015, 0.0950, 0.1062, 0.1036, 0.1081, 0.0907,\n",
            "           0.0999, 0.1037],\n",
            "          [0.0965, 0.1008, 0.1037, 0.0959, 0.1005, 0.0978, 0.1103, 0.0938,\n",
            "           0.0999, 0.1007]],\n",
            "\n",
            "         [[0.0988, 0.0978, 0.0994, 0.1003, 0.0980, 0.1023, 0.1015, 0.0990,\n",
            "           0.1075, 0.0952],\n",
            "          [0.0996, 0.0955, 0.0948, 0.1035, 0.0976, 0.1067, 0.0965, 0.1006,\n",
            "           0.1140, 0.0911],\n",
            "          [0.0984, 0.0929, 0.0921, 0.1047, 0.0992, 0.1073, 0.1011, 0.0998,\n",
            "           0.1114, 0.0930],\n",
            "          [0.1007, 0.0929, 0.0986, 0.1043, 0.0982, 0.1061, 0.0940, 0.0971,\n",
            "           0.1132, 0.0949],\n",
            "          [0.0987, 0.0964, 0.0916, 0.1004, 0.0940, 0.1076, 0.1007, 0.1033,\n",
            "           0.1080, 0.0993],\n",
            "          [0.0980, 0.0958, 0.0968, 0.1046, 0.0973, 0.1042, 0.0954, 0.0965,\n",
            "           0.1184, 0.0930],\n",
            "          [0.1005, 0.0963, 0.0998, 0.1008, 0.0963, 0.1073, 0.0943, 0.1042,\n",
            "           0.1068, 0.0936],\n",
            "          [0.0988, 0.0954, 0.0992, 0.1042, 0.0989, 0.1065, 0.1005, 0.0945,\n",
            "           0.1072, 0.0949],\n",
            "          [0.0999, 0.0951, 0.0997, 0.1049, 0.0941, 0.1035, 0.1010, 0.0997,\n",
            "           0.1138, 0.0883],\n",
            "          [0.0972, 0.1001, 0.0993, 0.0998, 0.0899, 0.1106, 0.0999, 0.0966,\n",
            "           0.1124, 0.0941]],\n",
            "\n",
            "         [[0.1005, 0.1041, 0.0995, 0.0998, 0.1088, 0.0966, 0.0985, 0.0945,\n",
            "           0.1008, 0.0969],\n",
            "          [0.1030, 0.1080, 0.1014, 0.0951, 0.1106, 0.0954, 0.0983, 0.0910,\n",
            "           0.0958, 0.1014],\n",
            "          [0.1025, 0.1041, 0.1033, 0.1023, 0.1089, 0.0981, 0.0984, 0.0881,\n",
            "           0.0971, 0.0973],\n",
            "          [0.0989, 0.1049, 0.1014, 0.0976, 0.1118, 0.0956, 0.1007, 0.0904,\n",
            "           0.1032, 0.0956],\n",
            "          [0.1027, 0.1098, 0.0959, 0.0988, 0.1092, 0.1002, 0.1004, 0.0871,\n",
            "           0.0987, 0.0972],\n",
            "          [0.0966, 0.1043, 0.1013, 0.0973, 0.1074, 0.1008, 0.1004, 0.0916,\n",
            "           0.0979, 0.1024],\n",
            "          [0.1023, 0.1081, 0.0981, 0.0985, 0.1130, 0.0967, 0.0956, 0.0891,\n",
            "           0.0997, 0.0990],\n",
            "          [0.1069, 0.1076, 0.1028, 0.0983, 0.1113, 0.0938, 0.0947, 0.0881,\n",
            "           0.0984, 0.0982],\n",
            "          [0.1003, 0.1073, 0.0995, 0.0987, 0.1078, 0.0909, 0.1035, 0.0995,\n",
            "           0.0986, 0.0940],\n",
            "          [0.1009, 0.1085, 0.0991, 0.1005, 0.1055, 0.0989, 0.1012, 0.0892,\n",
            "           0.0960, 0.1002]]]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Softmax\n",
        "\n",
        "# Define the parameters\n",
        "d_model = 512  # Dimension of the model\n",
        "n_heads = 8    # Number of attention heads\n",
        "seq_length = 10  # Length of the input sequence (number of tokens)\n",
        "\n",
        "# Random input tensor (batch_size, seq_length, d_model)\n",
        "x = torch.rand((1, seq_length, d_model))\n",
        "\n",
        "# Split the input tensor into multiple heads\n",
        "def split_heads(x, n_heads):\n",
        "    batch_size, seq_length, d_model = x.size()\n",
        "    depth = d_model // n_heads\n",
        "    x = x.view(batch_size, seq_length, n_heads, depth)\n",
        "    return x.permute(0, 2, 1, 3)\n",
        "\n",
        "# Scaled dot-product attention\n",
        "def scaled_dot_product_attention(q, k, v):\n",
        "    matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
        "    dk = q.size()[-1]\n",
        "    scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(dk, dtype=torch.float32))\n",
        "    attention_weights = Softmax(dim=-1)(scaled_attention_logits)\n",
        "    output = torch.matmul(attention_weights, v)\n",
        "    return output, attention_weights\n",
        "\n",
        "# Multi-head attention\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.wq = Linear(d_model, d_model)\n",
        "        self.wk = Linear(d_model, d_model)\n",
        "        self.wv = Linear(d_model, d_model)\n",
        "        self.dense = Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        q = self.wq(x)\n",
        "        k = self.wk(x)\n",
        "        v = self.wv(x)\n",
        "\n",
        "        q = split_heads(q, self.n_heads)\n",
        "        k = split_heads(k, self.n_heads)\n",
        "        v = split_heads(v, self.n_heads)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v)\n",
        "        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous()\n",
        "        original_size_attention = scaled_attention.view(batch_size, -1, self.d_model)\n",
        "\n",
        "        output = self.dense(original_size_attention)\n",
        "        return output, attention_weights\n",
        "\n",
        "mha = MultiHeadAttention(d_model, n_heads)\n",
        "output, attention_scores = mha(x)\n",
        "\n",
        "# Print the attention scores\n",
        "print(\"Attention Scores:\", attention_scores)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bYfDv5gHKeG-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}